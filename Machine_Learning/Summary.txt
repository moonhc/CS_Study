2016-08-19 Fri

<패턴인식> 소개

1장
패턴인식의 처리과정: 패턴->특징->분류->부류

데이터로부터 추출한 특징을 이미 알고있는 지식에 비추어 의사 결정e 분류

인식기를 만다는데 쓰이는 훈련 집합
       - 성능 평가를 위한 테스트 집합

이러한 샘플들의 집합이 database. 양적 질적으로 모두 우수해야 사용가능.

주어진 샘플에서 특징을 추출해 특징 벡터를 만든다. x = (x1, x2 ... xd)
특징 벡터의 크기가 커지면 메모리와 계산량이 커지므로 적당한 크기를 잡아주어야 한다.

분류기(classifier)는 특징 벡터들을 어떤 수학적 모델을 사용할 것인지 결정한 후, 이 모델을 기반으로 훈련 집합을 통해 분류기를 훈련시켜야 한다.

학습 과정은 지도학습(superviesd learning)과 비지도학습(unsupervised learning)으로 나뉜다.
지도 학습은 훈련 집합의 샘플들의 참 부류(true class)를 알고 있고 이를 이용하여 훈련을 하는 것이다.
비지도 학습은 샘플의 부류정보가 주어지지 않으므로, 비슷한 샘플들을 같은 집단으로 모으는 군집화(clustering)문제라고도 부른다.

패턴 인식 시스템의 성능 평가는 크게 두가지 방법이 있다.
1. 맞추거나 틀리거나 기각한 샘플들의 수를 세어 평가하는 방법
(정인식률(correct recognition reta), 기각률(rejection rate), 오류율(error rate)) - 단순 분류
2. 틀리게 분류했을 때, 그 위험도를 수치화하는 방법 - 병원진단
* 보통 시스템 별로 각자에 맞는 기준을 정립해 사용한다.

2장
사람은 의식과 무의식을 이용해 가장 그럴듯한 결정을 내리는 반면, 컴퓨터는 이런 결정을 위해 수학적 틀을 사용한다. 
조건부 확률을 이용한 사후확률: 특징벡터 x가 주어졌을 때, 이것이 부류wi인 경우는 p(wi|x)로 표현

Bayesian theorem:
		p(x,y) = p(y,x)
	p(x)p(y|x) = p(y)p(x|y)
		p(x|Y) = p(y|x)p(x) / p(y)

curse of dimension를 피하려면 naive Bayesian classifier를 사용

3장
2장에서는 사전확률과 우도를 알고 있다고 가정했다.
하지만 실제에서는 추정을 통해 알아내야 한다.

사전확률 추정은 비교적 쉽다. p(wi) = Ni / N

우도를 추정하는게 관건.

- 히스토그램 추정
	현실적으로 이 방법은 공간의 차원이 낮고 X의 크기가 충분히 커야한다.
- 최대 우도
	매개변수로 표현되는 경우에만 적용 가능
- 비모수적 방법
	매개변수로 표현되지 않는 특정 확률 분포를 위한 방법
	* 파젠 창: 히스토그램과 유사한 추정 방법
	* k-최근접 이웃 추정: k개의 샘플이 창에 들어올때 까지 창을 늘린다.
	* k-최근적 이웃 분류기
- 혼합모델
	두개 이상의 서로 다른 확률 분포의 모델링
	* 가우시안 혼합
	* EM 알고리즘

4장 
신경망: 정보처리를 위한 수학적 모델
인공 신경망(artificial neural network, ANN)
연결주의(connectionism)의 원리를 통해, 단순한 연산을 하는 연산기들과 그들 간의 수많은 연결을 통해 지능적인 일을 달성하는 계산모형

- 퍼셉트론
	선형 분류의 한계, 다층 퍼셉트론의 토대
	퍼셉트론은 입력 층과 출력 층 두가지 층을 가지고 있다. 
	입력 층은 d+1개의 노드를 가진다. d개의 노드는 특징 벡터의 성분과 연결되며 나머지 한개의 노드는 bias에 해당한다. 항상 1 값을 갖는다.
	출력 층은 하나의 노드를 갖는다.
	입력 노드와 출력 노드의 연결선은 가중치를 갖는다.
- 다층 퍼셉트론
	선형 분리가 불가능한 경우 퍼셉트론을 여러 층으로 이어 붙인 다층 퍼셉트론을 이용
	입력 층, 은닉 층, 출력 층
	출력 층 부류 개수의 노드, 은닉 층 사용자 지정 노드 개수 +1

	층의 개수, 층간의 연결, 각 층의 노드 개수, 활성 함수의 종류를 고려하여 설계

5장
SVM: 오류율 최소화하는 목적이 아닌 일반화를 극대화시키는 개념을 사용, 이진 분류기
x     |  x    x  x
 x  x |     x 
 xx   |  x xxx  x

* 이 경우 왼쪽 평면보다 오른쪽 평면이 여백을 더 크게 만들어줌

6장
질적 분류
- 결정트리
- CART, ID3, C4.5: 결정 트리를 활용한 시스템
- 스트링 인식기: 교정거리 => Levenshtein 거리 Damerau-Levenshtein 거리

7장
순차 데이터: 시간적 문맥적 의존성이 있는 데이터
은닉 마코프 모델(Hidden Markov Model, HMM)이 대표적인 모델

10장
거리와 유사도: Minkowski 거리



ref: 패턴인식 www.navisphere.net https://drive.google.com/open?id=0B9MJBJPl0R2PN05TcDNKQm44TGc
